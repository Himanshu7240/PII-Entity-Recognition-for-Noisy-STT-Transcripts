# PII Entity Recognition for Noisy STT Transcripts

A high-precision token-level NER model for detecting PII entities in noisy Speech-to-Text transcripts using DistilBERT.

## ğŸ¯ Results

| Metric | Value | Target | Status |
|--------|-------|--------|--------|
| **PII Precision** | **1.000** | â‰¥0.80 | âœ… Exceeded |
| **PII Recall** | **0.926** | - | âœ… Strong |
| **PII F1** | **0.961** | - | âœ… Excellent |
| **Macro-F1** | **0.944** | - | âœ… Excellent |
| **p50 Latency** | **14.40 ms** | - | âœ… Fast |
| **p95 Latency** | **27.86 ms** | â‰¤20 ms | âš ï¸ Above target |

**Key Achievement**: Perfect 1.000 PII precision with zero false positives on dev set.

## ğŸ“‹ Entity Types

**PII Entities (5):**
- CREDIT_CARD
- PHONE
- EMAIL
- PERSON_NAME
- DATE

**Non-PII Entities (2):**
- CITY
- LOCATION

## ğŸš€ Quick Start

### Installation

```bash
pip install -r requirements.txt
```

### Training

```bash
python src/train.py \
  --model_name distilbert-base-uncased \
  --train data/train_new.jsonl \
  --dev data/dev_new.jsonl \
  --out_dir out \
  --epochs 5 \
  --batch_size 16 \
  --lr 3e-5 \
  --dropout 0.3
```

### Prediction

```bash
# Dev set
python src/predict.py \
  --model_dir out \
  --input data/dev_new.jsonl \
  --output out/dev_pred.json \
  --confidence_threshold 0.5

# Stress test
python src/predict.py \
  --model_dir out \
  --input data/stress.jsonl \
  --output out/stress_pred.json \
  --confidence_threshold 0.5
```

### Evaluation

```bash
# Dev set
python src/eval_span_f1.py \
  --gold data/dev_new.jsonl \
  --pred out/dev_pred.json

# Stress test
python src/eval_span_f1.py \
  --gold data/stress.jsonl \
  --pred out/stress_pred.json
```

### Latency Measurement

```bash
python src/measure_latency.py \
  --model_dir out \
  --input data/dev_new.jsonl \
  --runs 50
```

## ğŸ“ Project Structure

```
â”œâ”€â”€ README.md                    # This file
â”œâ”€â”€ requirements.txt             # Dependencies
â”œâ”€â”€ FINAL_METRICS.md            # Detailed performance metrics
â”œâ”€â”€ APPROACH_COMMENTS.md        # Methodology and approach
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ train_new.jsonl         # Training data (700 examples)
â”‚   â”œâ”€â”€ dev_new.jsonl           # Dev data (160 examples)
â”‚   â”œâ”€â”€ stress.jsonl            # Stress test (100 examples)
â”‚   â””â”€â”€ test.jsonl              # Test data (175 examples)
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ train.py                # Training script
â”‚   â”œâ”€â”€ predict.py              # Prediction script
â”‚   â”œâ”€â”€ model.py                # Model definition
â”‚   â”œâ”€â”€ dataset.py              # Data loading
â”‚   â”œâ”€â”€ labels.py               # Label definitions
â”‚   â”œâ”€â”€ eval_span_f1.py         # Evaluation metrics
â”‚   â””â”€â”€ measure_latency.py      # Latency measurement
â”œâ”€â”€ scripts/
â”‚   â””â”€â”€ prepare_data.py         # Data preparation
â””â”€â”€ out/
    â”œâ”€â”€ dev_pred.json           # Dev predictions
    â”œâ”€â”€ stress_pred.json        # Stress predictions
    â”œâ”€â”€ test_pred.json          # Test predictions
    â””â”€â”€ config.json             # Model config
```

## ğŸ”§ Technical Details

### Model Architecture
- **Base Model**: DistilBERT-base-uncased (66M parameters)
- **Task**: Token Classification with BIO tagging
- **Framework**: PyTorch + Transformers

### Key Features

1. **Confidence-Based Filtering**
   - Filters predictions below confidence threshold (default: 0.5)
   - Achieves perfect precision by rejecting uncertain predictions

2. **Post-Processing Validation**
   - EMAIL: Must contain "at" or "@"
   - PHONE/CREDIT_CARD: Must contain digits
   - Minimum span length: 2 characters

3. **Enhanced Dropout**
   - Dropout: 0.3 (higher than default)
   - Better generalization on noisy STT data

4. **Optimized Training**
   - 5 epochs with AdamW optimizer
   - Linear warmup schedule
   - Batch size: 16, Learning rate: 3e-5

### Hyperparameters

```python
model_name = "distilbert-base-uncased"
epochs = 5
batch_size = 16
learning_rate = 3e-5
dropout = 0.3
max_length = 256
confidence_threshold = 0.5
```

## ğŸ“Š Performance Details

### Dev Set (160 examples)

| Entity | Precision | Recall | F1 | PII |
|--------|-----------|--------|-----|-----|
| PERSON_NAME | 1.000 | 1.000 | 1.000 | âœ“ |
| EMAIL | 1.000 | 1.000 | 1.000 | âœ“ |
| DATE | 1.000 | 1.000 | 1.000 | âœ“ |
| CREDIT_CARD | 1.000 | 0.708 | 0.829 | âœ“ |
| PHONE | 1.000 | 0.635 | 0.776 | âœ“ |
| CITY | 1.000 | 1.000 | 1.000 | âœ— |
| LOCATION | 1.000 | 1.000 | 1.000 | âœ— |

### Training Progress

| Epoch | Loss |
|-------|------|
| 1 | 1.5228 |
| 2 | 0.2242 |
| 3 | 0.0509 |
| 4 | 0.0263 |
| 5 | 0.0213 |

Loss reduced by 98.6% showing excellent convergence.

## ğŸ¯ Design Decisions

### 1. Precision over Recall
- **Decision**: Prioritize precision (achieved 1.000)
- **Rationale**: False positives in PII detection are costly
- **Trade-off**: Slightly lower recall on PHONE (0.635) and CREDIT_CARD (0.708)

### 2. DistilBERT over Smaller Models
- **Decision**: Use DistilBERT (66M params)
- **Rationale**: Balance between accuracy and speed
- **Trade-off**: p95 latency 27.86 ms (above 20 ms target)

### 3. Confidence Threshold 0.5
- **Decision**: Default threshold of 0.5
- **Rationale**: Balanced precision/recall
- **Tunable**: Can adjust for different use cases

## ğŸ”® Future Improvements

### For Better Latency (to reach p95 â‰¤ 20ms)
1. Use smaller model (BERT-tiny, ALBERT-base)
2. ONNX runtime optimization
3. Quantization (INT8)
4. Model distillation

### For Better Stress Test Performance
1. Add adversarial training examples
2. Data augmentation (typos, variations)
3. Ensemble methods
4. More validation rules

## ğŸ“š Documentation

- **[FINAL_METRICS.md](FINAL_METRICS.md)** - Detailed performance metrics
- **[APPROACH_COMMENTS.md](APPROACH_COMMENTS.md)** - Methodology and design decisions
- **[RUN_COMMANDS.md](RUN_COMMANDS.md)** - Command reference
- **[CODE_CHANGES_SUMMARY.md](CODE_CHANGES_SUMMARY.md)** - Code modifications

## ğŸ› ï¸ Requirements

```
torch
transformers
numpy
tqdm
seqeval
```

## ğŸ“ Output Format

Predictions are in JSON format:

```json
{
  "utt_0012": [
    {
      "start": 3,
      "end": 19,
      "label": "CREDIT_CARD",
      "pii": true
    },
    {
      "start": 63,
      "end": 77,
      "label": "PERSON_NAME",
      "pii": true
    }
  ]
}
```

## ğŸ† Highlights

- âœ… **Perfect PII Precision**: 1.000 (zero false positives)
- âœ… **Strong F1 Score**: 0.961 PII F1
- âœ… **Fast Inference**: 14.40 ms p50 latency
- âœ… **Production-Ready**: Comprehensive validation and error handling
- âœ… **Well-Documented**: Detailed metrics and approach documentation

## ğŸ“„ License

This project is for educational purposes.

## ğŸ‘¤ Author

[Your Name]

## ğŸ™ Acknowledgments

- Assignment from IIT Bombay
- Built with PyTorch and Hugging Face Transformers
- DistilBERT model by Hugging Face
